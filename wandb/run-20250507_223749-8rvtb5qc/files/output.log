(450, 13) (50, 13)
[34m[1mwandb[0m: wandb.init() called while a run is active and reinit is set to 'default', so returning the previous run.
train_loss=10.240, state pred rel l2 error: 4.109, cumulative reward pred abs error: 0.899
train_loss=6.742, state pred rel l2 error: 3.673, cumulative reward pred abs error: 0.884
train_loss=4.023, state pred rel l2 error: 2.797, cumulative reward pred abs error: 0.865
train_loss=3.737, state pred rel l2 error: 3.917, cumulative reward pred abs error: 0.852
train_loss=1.884, state pred rel l2 error: 2.460, cumulative reward pred abs error: 0.839
train_loss=1.220, state pred rel l2 error: 2.582, cumulative reward pred abs error: 0.824
train_loss=0.525, state pred rel l2 error: 4.589, cumulative reward pred abs error: 0.809
train_loss=0.272, state pred rel l2 error: 1.511, cumulative reward pred abs error: 0.800
train_loss=-1.639, state pred rel l2 error: 3.801, cumulative reward pred abs error: 0.787
train_loss=-1.096, state pred rel l2 error: 1.980, cumulative reward pred abs error: 0.781
train_loss=-1.284, state pred rel l2 error: 1.736, cumulative reward pred abs error: 0.774
train_loss=-2.518, state pred rel l2 error: 3.585, cumulative reward pred abs error: 0.766
train_loss=-2.740, state pred rel l2 error: 2.569, cumulative reward pred abs error: 0.748
train_loss=-3.848, state pred rel l2 error: 1.979, cumulative reward pred abs error: 0.729
train_loss=-3.823, state pred rel l2 error: 2.056, cumulative reward pred abs error: 0.720
train_loss=-2.633, state pred rel l2 error: 1.834, cumulative reward pred abs error: 0.699
train_loss=-4.027, state pred rel l2 error: 1.320, cumulative reward pred abs error: 0.675
train_loss=-4.603, state pred rel l2 error: 0.701, cumulative reward pred abs error: 0.654
train_loss=-6.129, state pred rel l2 error: 1.380, cumulative reward pred abs error: 0.629
train_loss=-4.329, state pred rel l2 error: 0.836, cumulative reward pred abs error: 0.603
train_loss=-3.558, state pred rel l2 error: 1.351, cumulative reward pred abs error: 0.568
train_loss=-2.971, state pred rel l2 error: 2.750, cumulative reward pred abs error: 0.520
train_loss=-4.380, state pred rel l2 error: 1.935, cumulative reward pred abs error: 0.495
train_loss=-3.449, state pred rel l2 error: 2.740, cumulative reward pred abs error: 0.448
train_loss=-1.667, state pred rel l2 error: 0.790, cumulative reward pred abs error: 0.415
train_loss=-3.545, state pred rel l2 error: 1.309, cumulative reward pred abs error: 0.366
train_loss=-0.788, state pred rel l2 error: 1.248, cumulative reward pred abs error: 0.335
train_loss=-0.905, state pred rel l2 error: 2.091, cumulative reward pred abs error: 0.308
train_loss=-2.035, state pred rel l2 error: 3.628, cumulative reward pred abs error: 0.279
train_loss=-0.703, state pred rel l2 error: 2.035, cumulative reward pred abs error: 0.273
train_loss=21.035, state pred rel l2 error: 3.314, cumulative reward pred abs error: 0.278
train_loss=-0.324, state pred rel l2 error: 1.703, cumulative reward pred abs error: 0.273
train_loss=-1.624, state pred rel l2 error: 2.065, cumulative reward pred abs error: 0.263
train_loss=1.857, state pred rel l2 error: 1.566, cumulative reward pred abs error: 0.242
train_loss=-1.553, state pred rel l2 error: 1.177, cumulative reward pred abs error: 0.247
train_loss=-7.238, state pred rel l2 error: 1.724, cumulative reward pred abs error: 0.248
train_loss=2.764, state pred rel l2 error: 3.367, cumulative reward pred abs error: 0.274
train_loss=0.338, state pred rel l2 error: 3.306, cumulative reward pred abs error: 0.264
